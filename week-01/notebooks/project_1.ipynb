{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: if you feel like your Pandas skills need a bit of a touch up, check [this article out](https://levelup.gitconnected.com/20-pandas-functions-for-80-of-your-data-science-tasks-b610c8bfe63c)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we're going to be using is one of the most realistic retail time-series datasets you'll find out there because, well, it's actual Walmart data. It was made available for a Kaggle competition that you can check out [here](https://www.kaggle.com/c/m5-forecasting-accuracy).\n",
    "\n",
    "The original format of the data was in a \"wide\" format to made it smaller in memory, but that doesn't really work too well with databases and you won't see that very often in the real world. The most notable changes are that I added in a date column to replace the date identifier columns that were previously there, and I made the data smaller by only subsetting to the state of Texas.\n",
    "\n",
    "**Another note:** If you want to develop and test your code with a smaller dataset (which I'd probably recommend), set `sampled` in the cell below to `True`. All of the tests will still pass if your code is correct!\n",
    "\n",
    "Let's get into it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-13T20:01:57.274707Z",
     "start_time": "2023-03-13T20:01:57.257630Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = '../data/'\n",
    "sampled = True\n",
    "\n",
    "path_suffix = '' if not sampled else '_sampled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-13T20:01:57.711152Z",
     "start_time": "2023-03-13T20:01:57.450001Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting our data in the right format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time-series data has to be collected from some real-world, data-generating process. That means that raw data comes in as a series of observations. Depending on your experience with time-series data, you may be used to data that looks like this:\n",
    "\n",
    "| Date       | Sales |\n",
    "|------------|-------|\n",
    "| 2022-01-01 |  23   |\n",
    "| 2022-01-02 |  45   |\n",
    "| 2022-01-03 |  12   |\n",
    "| 2022-01-04 |  67   |\n",
    "| 2022-01-05 |  89   |\n",
    "\n",
    "But, if you're in retail, each of those \"sales\" probably came in some JSON from some point-of-sale system (i.e. cash register) that probably looked something like this:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"timestamp\": 2022-01-01 12:34:56,\n",
    "    \"product_id\": 5,\n",
    "    \"store_id\": 12,\n",
    "    \"category_id\": 36,\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "Usually, it's the job of a data engineer to collect all of these records and aggregate them into a nice, tabular format, but it's worth at least having an appreciation for how it's done. So, we're going to start from a mock version of a transactions table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-13T20:02:16.484230Z",
     "start_time": "2023-03-13T20:02:16.364925Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01 13:41:03</td>\n",
       "      <td>HOBBIES_1_004_TX_1_evaluation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>TX_1</td>\n",
       "      <td>TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-01 07:30:52</td>\n",
       "      <td>HOBBIES_1_004_TX_1_evaluation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>TX_1</td>\n",
       "      <td>TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-01 11:17:38</td>\n",
       "      <td>HOBBIES_1_004_TX_1_evaluation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>TX_1</td>\n",
       "      <td>TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-01 06:07:58</td>\n",
       "      <td>HOBBIES_1_004_TX_2_evaluation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>TX_2</td>\n",
       "      <td>TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-01 21:51:07</td>\n",
       "      <td>HOBBIES_1_004_TX_2_evaluation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>TX_2</td>\n",
       "      <td>TX</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date                             id        item_id  \\\n",
       "0  2013-01-01 13:41:03  HOBBIES_1_004_TX_1_evaluation  HOBBIES_1_004   \n",
       "1  2013-01-01 07:30:52  HOBBIES_1_004_TX_1_evaluation  HOBBIES_1_004   \n",
       "2  2013-01-01 11:17:38  HOBBIES_1_004_TX_1_evaluation  HOBBIES_1_004   \n",
       "3  2013-01-01 06:07:58  HOBBIES_1_004_TX_2_evaluation  HOBBIES_1_004   \n",
       "4  2013-01-01 21:51:07  HOBBIES_1_004_TX_2_evaluation  HOBBIES_1_004   \n",
       "\n",
       "     dept_id   cat_id store_id state_id  \n",
       "0  HOBBIES_1  HOBBIES     TX_1       TX  \n",
       "1  HOBBIES_1  HOBBIES     TX_1       TX  \n",
       "2  HOBBIES_1  HOBBIES     TX_1       TX  \n",
       "3  HOBBIES_1  HOBBIES     TX_2       TX  \n",
       "4  HOBBIES_1  HOBBIES     TX_2       TX  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions = pd.read_csv(f'{data_dir}/transactions_data{path_suffix}.csv')\n",
    "\n",
    "transactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-13T20:02:17.314491Z",
     "start_time": "2023-03-13T20:02:17.305683Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date        object\n",
       "id          object\n",
       "item_id     object\n",
       "dept_id     object\n",
       "cat_id      object\n",
       "store_id    object\n",
       "state_id    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that this is a DataFrame where each row relates to purchases for an individual item. Here's a little data dictionary:\n",
    "\n",
    "- `date`: the time at which an item was bought, down to the second\n",
    "- `id`: the product ID. Each of these is an individual item at a specific store.\n",
    "- `item_id`: this is an identifier for items, but not at the store level. You can use this to find the same item at different stores.\n",
    "- `dept_id`: department ID. One level up from `item_id` in the hierarchy\n",
    "- `cat_id`: category ID. One level up from `dept_id` in the hierarchy\n",
    "- `store_id`: identifies the specific store where the product was bought\n",
    "- `state_id`: identifies the specific state where the product was bought\n",
    "\n",
    "`date` is supposed to be a `datetime`-like object, but you can see that when we loaded it from disk, it was loaded in as a string. Let's convert that column to `datetime`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-13T20:02:26.611897Z",
     "start_time": "2023-03-13T20:02:26.573364Z"
    }
   },
   "outputs": [],
   "source": [
    "# QUESTION: Convert this column to a datetime object\n",
    "transactions['date'] = pd.to_datetime(transactions['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-13T20:02:26.781633Z",
     "start_time": "2023-03-13T20:02:26.763210Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01 13:41:03</td>\n",
       "      <td>HOBBIES_1_004_TX_1_evaluation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>TX_1</td>\n",
       "      <td>TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-01 07:30:52</td>\n",
       "      <td>HOBBIES_1_004_TX_1_evaluation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>TX_1</td>\n",
       "      <td>TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-01 11:17:38</td>\n",
       "      <td>HOBBIES_1_004_TX_1_evaluation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>TX_1</td>\n",
       "      <td>TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-01 06:07:58</td>\n",
       "      <td>HOBBIES_1_004_TX_2_evaluation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>TX_2</td>\n",
       "      <td>TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-01 21:51:07</td>\n",
       "      <td>HOBBIES_1_004_TX_2_evaluation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>TX_2</td>\n",
       "      <td>TX</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date                             id        item_id  \\\n",
       "0 2013-01-01 13:41:03  HOBBIES_1_004_TX_1_evaluation  HOBBIES_1_004   \n",
       "1 2013-01-01 07:30:52  HOBBIES_1_004_TX_1_evaluation  HOBBIES_1_004   \n",
       "2 2013-01-01 11:17:38  HOBBIES_1_004_TX_1_evaluation  HOBBIES_1_004   \n",
       "3 2013-01-01 06:07:58  HOBBIES_1_004_TX_2_evaluation  HOBBIES_1_004   \n",
       "4 2013-01-01 21:51:07  HOBBIES_1_004_TX_2_evaluation  HOBBIES_1_004   \n",
       "\n",
       "     dept_id   cat_id store_id state_id  \n",
       "0  HOBBIES_1  HOBBIES     TX_1       TX  \n",
       "1  HOBBIES_1  HOBBIES     TX_1       TX  \n",
       "2  HOBBIES_1  HOBBIES     TX_1       TX  \n",
       "3  HOBBIES_1  HOBBIES     TX_2       TX  \n",
       "4  HOBBIES_1  HOBBIES     TX_2       TX  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-13T20:02:39.700790Z",
     "start_time": "2023-03-13T20:02:39.691835Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date        datetime64[ns]\n",
       "id                  object\n",
       "item_id             object\n",
       "dept_id             object\n",
       "cat_id              object\n",
       "store_id            object\n",
       "state_id            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to transform this dataset into one that's easy to analyze and train models on. For this project, our goal is going to be to work at the daily level. So, our first step is to aggregate our transactions data up to the daily level.\n",
    "\n",
    "To be more specific, this is what we want it to look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-13T20:02:40.565024Z",
     "start_time": "2023-03-13T20:02:40.547226Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>HOBBIES_1_004_TX_1_evaluation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>TX_1</td>\n",
       "      <td>TX</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>HOBBIES_2_075_TX_1_evaluation</td>\n",
       "      <td>HOBBIES_2_075</td>\n",
       "      <td>HOBBIES_2</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>TX_1</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>HOUSEHOLD_1_247_TX_1_evaluation</td>\n",
       "      <td>HOUSEHOLD_1_247</td>\n",
       "      <td>HOUSEHOLD_1</td>\n",
       "      <td>HOUSEHOLD</td>\n",
       "      <td>TX_1</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>HOUSEHOLD_1_266_TX_1_evaluation</td>\n",
       "      <td>HOUSEHOLD_1_266</td>\n",
       "      <td>HOUSEHOLD_1</td>\n",
       "      <td>HOUSEHOLD</td>\n",
       "      <td>TX_1</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>FOODS_1_001_TX_1_evaluation</td>\n",
       "      <td>FOODS_1_001</td>\n",
       "      <td>FOODS_1</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>TX_1</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                               id          item_id      dept_id  \\\n",
       "0  2013-01-01    HOBBIES_1_004_TX_1_evaluation    HOBBIES_1_004    HOBBIES_1   \n",
       "1  2013-01-01    HOBBIES_2_075_TX_1_evaluation    HOBBIES_2_075    HOBBIES_2   \n",
       "2  2013-01-01  HOUSEHOLD_1_247_TX_1_evaluation  HOUSEHOLD_1_247  HOUSEHOLD_1   \n",
       "3  2013-01-01  HOUSEHOLD_1_266_TX_1_evaluation  HOUSEHOLD_1_266  HOUSEHOLD_1   \n",
       "4  2013-01-01      FOODS_1_001_TX_1_evaluation      FOODS_1_001      FOODS_1   \n",
       "\n",
       "      cat_id store_id state_id  sales  \n",
       "0    HOBBIES     TX_1       TX      3  \n",
       "1    HOBBIES     TX_1       TX      0  \n",
       "2  HOUSEHOLD     TX_1       TX      0  \n",
       "3  HOUSEHOLD     TX_1       TX      0  \n",
       "4      FOODS     TX_1       TX      0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a hefty table, so just peeking at the first 5 rows\n",
    "pd.read_csv(f'{data_dir}/sales_data{path_suffix}.csv', nrows=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the `sales` column is really just a daily count of transactions for that particular `id`.\n",
    "\n",
    "In the cell below, create a dataframe called `data`, which is the transactions dataframe aggregated to the daily level. It should look like the above, except you won't have zero sales days. Don't worry about order: the below test will handle that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-13T20:02:51.430930Z",
     "start_time": "2023-03-13T20:02:51.426349Z"
    }
   },
   "outputs": [],
   "source": [
    "data = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-13T20:09:00.110935Z",
     "start_time": "2023-03-13T20:09:00.052807Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date        id                             \n",
       "2013-01-01  FOODS_3_714_TX_2_evaluation        10\n",
       "            FOODS_3_714_TX_3_evaluation        12\n",
       "            HOBBIES_1_004_TX_1_evaluation       3\n",
       "            HOBBIES_1_004_TX_2_evaluation       2\n",
       "            HOBBIES_1_004_TX_3_evaluation       1\n",
       "                                               ..\n",
       "2016-05-22  HOUSEHOLD_1_247_TX_2_evaluation     4\n",
       "            HOUSEHOLD_1_247_TX_3_evaluation     3\n",
       "            HOUSEHOLD_1_266_TX_1_evaluation     1\n",
       "            HOUSEHOLD_1_266_TX_2_evaluation     1\n",
       "            HOUSEHOLD_1_266_TX_3_evaluation     1\n",
       "Name: id, Length: 13345, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions.groupby(by = [pd.Grouper(key = 'date', freq = 'D'), 'id'])['id'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the cell below runs without error, you did it right!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-13T19:52:40.620077Z",
     "start_time": "2023-03-13T19:52:40.544088Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_sales_eq(data):\n",
    "    assert (\n",
    "        pd.read_csv(f'{data_dir}/sales_data{path_suffix}.csv', usecols=['date', 'id', 'sales'])\n",
    "        .assign(date=lambda df: pd.to_datetime(df.date))\n",
    "        .query('sales != 0')\n",
    "        .merge(data, on=['date', 'id'], how='left', suffixes=('_actual', '_predicted'))\n",
    "        .fillna(0)\n",
    "        .assign(sales_error=lambda df: (df.sales_actual - df.sales_predicted).abs())\n",
    "        .sales_error\n",
    "        .sum() < 1e-6\n",
    "    ), 'Your version of sales does not match the original sales data.'\n",
    "\n",
    "test_sales_eq(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how our data is being stored in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.5 GB of data for our purposed is certainly no joke. But how much of that is really necessary?\n",
    "\n",
    "Most of our data is stored in the least memory efficient format for pandas: strings (objects). Let's fix that.\n",
    "\n",
    "Hint: check out [this page](https://pandas.pydata.org/docs/user_guide/basics.html#basics-dtypes) of the pandas documentation that talks about data types.\n",
    "\n",
    "In the below cell, convert the data types of columns to reduce memory usage as much as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my solution, I got the final DataFrame down to 90.4 MB, which is about 6% of the original size!\n",
    "\n",
    "While we're at it, it's worth talking about the best way to store this data on disk. If we saved this as a CSV, it wouldn't maintain any of the data type modifications we just made. Pandas offers a bunch of options for saving DataFrames, but here are the two I'd recommend:\n",
    "\n",
    "- [Parquet](https://pandas.pydata.org/docs/dev/reference/api/pandas.DataFrame.to_parquet.html) has basically become the industry standard for storing tabular data on disk. It's a columnar file format that automatically compresses your data (which it does really well) and will maintain any data types you use in Pandas, with only a couple exceptions.\n",
    "\n",
    "- [Feather](https://pandas.pydata.org/docs/dev/reference/api/pandas.DataFrame.to_parquet.html) is also a columnar data format, but it optimizes heavily for read speed. Your file size will be much bigger than Parquets, but it's really useful when you need to heavily optimize for data reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_parquet('data/sales_data_checkpoint.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet('data/sales_data_checkpoint.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On my local machine, loading our original CSV took ~8.7 seconds, and that only took 0.1 seconds. And our data types were maintained! Nice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finishing up our data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's one last modification we need to make to our data before it's ready to go. The way that we converted transactions into sales was *slightly* problematic because now, when a product doesn't sell it just isn't present in our data, rather than appearing as a zero. \n",
    "\n",
    "That's an issue for our forecasting models, so let's fix it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, set your index to columns that the DataFrame is distinct on (`date` and `id`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create a MultiIndex with all combinations of daily dates and `id`s using `pd.MultiIndex.from_product`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "index_to_select = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, fill the resulting `NaN`s in your dataframe. Hint: it's tempting to use `.groupby().fillna(method='ffill')` (and backfilling), but unfortunately this method is quite slow on grouped data. I'd recommend manually recreating the categorical columns by splitting the `id` column on underscores. This cell could take over a minute to run depending on how you implement it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sales_eq(data):\n",
    "    assert (\n",
    "        pd.read_csv(f'{data_dir}/sales_data{path_suffix}.csv', usecols=['date', 'id', 'sales'])\n",
    "        .assign(date=lambda df: pd.to_datetime(df.date))\n",
    "        .merge(data, on=['date', 'id'], how='left', suffixes=('_actual', '_predicted'))\n",
    "        .fillna(0)\n",
    "        .assign(sales_error=lambda df: (df.sales_actual - df.sales_predicted).abs())\n",
    "        .sales_error\n",
    "        .sum() < 1e-6\n",
    "    ), 'Your version of sales does not match the original sales data.'\n",
    "\n",
    "test_sales_eq(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory data analysis is crucial for building the best models.\n",
    "\n",
    "Before you start this section, though, I would **highly recommend** that you set the index of your DataFrame to be on both the `date` and `id` field (our DataFrame has one row for each `date`/`id` combo). It's up to you, but it's good practice!\n",
    "\n",
    "For this section, find 3-5 insights about the data that you feel are helpful for building models. Specifically, we'll be building models at the `date`/`dept_id` level (i.e., a forecast for `FOODS_1` on 2011-02-01, 2011-02-02, etc., a forecast for `HOBBIES_1` on 2011-02-01, 2011-02-02, etc.)\n",
    "\n",
    "The only required one is an [autocorrelation analysis](https://pandas.pydata.org/docs/reference/api/pandas.plotting.autocorrelation_plot.html). Other than that, some ideas are:\n",
    "\n",
    "- Looking for seasonal patterns and trends for each department\n",
    "- Department sales by day of week\n",
    "- Analyses at a higher level, like the category level\n",
    "\n",
    "Anything goes! Be creative!\n",
    "\n",
    "Here's an example of plotting the category-level sales for `FOODS_1` to get you started:\n",
    "\n",
    "```\n",
    "(\n",
    "    data\n",
    "    .groupby(['date', 'dept_id'])\n",
    "    .sales\n",
    "    .sum()\n",
    "    [:, 'FOODS_1']\n",
    "    .plot()\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some plots!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training some models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can train some models!\n",
    "\n",
    "We're going to use the [statsforecast](https://github.com/Nixtla/statsforecast) library, since it makes training statistical time-series models really easy. There are other great libraries (like [darts](https://unit8co.github.io/darts/), which is more mature of a package) but I like `statsforecast` a bit more for these models. Eventually, we'll get to training our own models from scratch.\n",
    "\n",
    "Here's what you need to do:\n",
    "\n",
    "1. Aggregate sales up to the `date`/`dept_id` level so each date has 7 distinct records (one for each `dept_id`).\n",
    "2. Convert your aggregated data into the [format that `statsforecast` likes](https://nixtla.github.io/statsforecast/examples/getting_started_short.html).\n",
    "3. Fit and evaluate some models! This part is made straightforward by `statsforecast` so feel free to fit whatever you want, but focus on models like [this one](https://nixtla.github.io/statsforecast/models.html#arima-family) and [this one](https://nixtla.github.io/statsforecast/models.html#holt-winters-method) since we discussed them. Their documentation has a [quickstart](https://nixtla.github.io/statsforecast/examples/getting_started_short.html) to get going. I provided you with some helper code below to get started.\n",
    "    - Play around with `ARIMA` and `HoltWinters` and tune them using your intution, then compare them to [`AutoARIMA`](https://nixtla.github.io/statsforecast/models.html#autoarima) and [`AutoETS`](https://nixtla.github.io/statsforecast/models.html#autoets), which do the tuning for you!\n",
    "4. (If your time permits) try out other libraries! Go try to fit a [Prophet](https://facebook.github.io/prophet/docs/quick_start.html#python-api) model, fit some models using `darts` and see how they compare, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't worry about any error outputs you see from this\n",
    "! pip install statsforecast==1.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsforecast import StatsForecast\n",
    "from statsforecast.models import ARIMA, HoltWinters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate data to date/dept_id level\n",
    "\n",
    "train_data = data.groupby(['date', 'dept_id']).sales.agg('sum').reset_index()\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_data.rename(columns={\n",
    "    'dept_id': 'unique_id',\n",
    "    'date': 'ds',\n",
    "    'sales': 'y'\n",
    "})\n",
    "train_df = df[df.ds < pd.Timestamp('2016-04-24')]\n",
    "\n",
    "sf = StatsForecast(\n",
    "    models=[\n",
    "        # SARIMA(1, 1, 1)(1, 1, 1),7\n",
    "        ARIMA(order=(1, 1, 1), seasonal_order=(1, 1, 1), season_length=7),\n",
    "        # ETS model\n",
    "        HoltWinters(season_length=7)\n",
    "    ],\n",
    "    freq='D'\n",
    ")\n",
    "sf.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_df = sf.predict(h=28)\n",
    "forecast_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are `plotly` charts, which are interactive. If you're not familiar, you can:\n",
    "- Click a series name in the legend (on the right) to activate/deactivate it in the plots\n",
    "- Hover your mouse over the plots and click + drag to zoom in\n",
    "- Double click on the plots to zoom back out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.plot(df, forecast_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "5768b04258121350f986a32a10c2b5b63ea833426012d4b5b8a887aeeef377c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
