{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3\n",
    "\n",
    "We've built low-level models, and we've built high-level models. Now, our goal is two-fold:\n",
    "\n",
    "- Build low-level models into better high-level models, and vice-versa.\n",
    "- Ensemble our models to make them more reliable\n",
    "\n",
    "The process for performing hierarchical reconciliation is laid out well [here](https://nixtla.github.io/hierarchicalforecast/examples/tourismsmall.html). I'd recommend following along!\n",
    "\n",
    "Let's get going!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install statsforecast mlforecast hierarchicalforecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = ''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to start by building a model at the store-deparment level. Our goal is to create a forecast at that level that coherently aggregates up to the state level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (\n",
    "    pd.read_parquet(f'{data_dir}/sales_data.parquet')\n",
    "    .reset_index()\n",
    "    .rename(columns={'date': 'ds', 'sales': 'y'})\n",
    "    .assign(store_dept_id=lambda df: df.store_id + '_' + df.dept_id)\n",
    "    .groupby(['ds', 'store_dept_id', 'store_id', 'state_id'])\n",
    "    .y\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to set up our hierarchical data. Use the `aggregate` method from `hierarchicalforecast` to hierarchically structure our data and get the proper summing dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hierarchicalforecast.utils import aggregate\n",
    "\n",
    "# this is the `spec` argument in the `aggregate` function\n",
    "hierarchy_levels = [['state_id'],\n",
    "                    ['state_id', 'store_id'],\n",
    "                    ['state_id', 'store_id', 'store_dept_id']]\n",
    "y_hier, S_df, tags = aggregate(...)\n",
    "y_hier = y_hier.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hierarchicalforecast.utils import HierarchicalPlot\n",
    "\n",
    "HierarchicalPlot(S_df, tags).plot_summing_matrix()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I went ahead and split your data for you. Train a `StatsForecast` model (any algorithm works, I used AutoARIMA). Be sure to grab the fitted values (i.e. the predictions on the training set), since we'll need that later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = y_hier.groupby('unique_id').tail(28)\n",
    "train = y_hier.drop(val.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsforecast import StatsForecast\n",
    "\n",
    "fcst = StatsForecast(...)\n",
    "y_fcst = fcst.forecast(h=28, fitted=True) # forecast on the validation period\n",
    "y_fitted = fcst.forecast_fitted_values() # fitted values, i.e. forecast on the training data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `StatsForecast.plot` method to visualize your predictions. Try passing `plot_random=False` to see how the model performs at the top levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statsforecast plots go here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to reconcile! Use the BottomUp, TopDown, and MinTrace reconciliation methods. For TopDown and MinTrace, try out the different methods provided by `hierarchicalforecast` to see which ones work best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hierarchicalforecast.core import HierarchicalReconciliation\n",
    "from hierarchicalforecast.methods import BottomUp, TopDown, MinTrace\n",
    "\n",
    "reconcilers = [\n",
    "    ...\n",
    "]\n",
    "hrec = HierarchicalReconciliation(reconcilers=reconcilers)\n",
    "\n",
    "y_rec = hrec.reconcile(\n",
    "    ...\n",
    ")\n",
    "y_rec.groupby('unique_id').head(2).head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results for your raw model predictions against the hierarchical predictions. How closely do the direct, non-hierarchical forecasts agree/disagree? What about the hierarchical forecasts?\n",
    "\n",
    "Hint: the below plot shows a sample, but the answer to this question lies with `plot_hierarchical_predictions_gap()`. Check out some of the other plotting methods, too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HierarchicalPlot(S_df, tags).plot_hierarchically_linked_series(\n",
    "    bottom_series='TX/TX_3/TX_3_HOUSEHOLD_2',\n",
    "    Y_df=val.merge(y_rec, on=['unique_id', 'ds']).set_index('unique_id'),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the helper code below to calculate RMSSE for every method you tried, at every level of the hierarchy. This code is borrowed and modified from the `hierarchicalforecast` library, because their implementation of `msse` is different than our version. This formulation of RMSSE should line up with the formulation from Project 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hierarchicalforecast.evaluation import HierarchicalEvaluation\n",
    "\n",
    "def mse(y, y_hat, weights=None, axis=None):\n",
    "    delta_y = np.square(y - y_hat)\n",
    "    if weights is not None:\n",
    "        mse = np.average(delta_y,\n",
    "                         weights=weights,\n",
    "                         axis=axis)\n",
    "    else:\n",
    "        mse = np.nanmean(delta_y, axis=axis)\n",
    "    return mse\n",
    "\n",
    "def rmsse(y, y_hat, y_insample, mask=None, insample_mask=None):\n",
    "    if mask is None: \n",
    "       mask = np.ones_like(y)\n",
    "       \n",
    "    eps = np.finfo(float).eps\n",
    "\n",
    "    norm = mse(y=y_insample[:, 1:], y_hat=y_insample[:, :-1], weights=insample_mask, axis=1)\n",
    "    loss = mse(y=y, y_hat=y_hat, weights=mask, axis=1)\n",
    "\n",
    "    loss = np.sqrt(loss / (norm + eps))\n",
    "    return loss.mean()\n",
    "\n",
    "HierarchicalEvaluation([rmsse]).evaluate(\n",
    "    Y_hat_df=..., # your reconciled forecasts\n",
    "    Y_test_df=..., # validation actuals DF, with unique_id as index\n",
    "    tags=tags, # tags from aggregate()\n",
    "    Y_df=... # training actuals DF, with unique_id as index\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions:\n",
    "\n",
    "- What's more accurate, the direct forecast, or the hierarchical methods?\n",
    "- What's the most accurate method for top-level aggregation (i.e. the `state_id` level)?\n",
    "- What's the most accurate method for bottom-level aggregation (i.e. the `state_id`/`store_id`/`store_dept_id` level)?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try to ensemble predictions from multiple models.\n",
    "\n",
    "Below, fit two models -- one `mlforecast` model (could be the same one you used in Project 2), and one `statsforecast` model. You can fit the models at any level you want (just make sure both are fit at the same level), but I'd recommend trying out `item_id`. It's a little faster than at the `id` level, and it gives both models a good opportunity to show their diversity.\n",
    "\n",
    "Once you've fit both models, be sure to plot some sample predictions.\n",
    "\n",
    "Don't worry about tuning the performance much here. This is more about seeing ensembling in action than optimizing your individual models!\n",
    "\n",
    "[Optional, if you have extra time] You can also fit a `neuralforecast` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in a fresh copy of the data\n",
    "data = (\n",
    "    pd.read_parquet('data/sales_data.parquet')\n",
    "    .reset_index()\n",
    "    .groupby(['date', 'item_id', 'dept_id', 'cat_id'])\n",
    "    .sales\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .assign(unique_id=lambda df: df.item_id.copy())\n",
    "    .rename(columns={'date': 'ds', 'sales': 'y'})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit an mlforecast model\n",
    "\n",
    "from mlforecast import MLForecast\n",
    "\n",
    "models = [\n",
    "    ...\n",
    "]\n",
    "\n",
    "fcst = MLForecast(\n",
    "    ...\n",
    ")\n",
    "\n",
    "fcst.fit(\n",
    "    ...\n",
    ")\n",
    "preds_ml = fcst.predict(28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a statsforecast model\n",
    "\n",
    "from statsforecast.core import StatsForecast\n",
    "\n",
    "fcst = StatsForecast(\n",
    "    ...\n",
    ")\n",
    "# preds_sf = fcst.forecast(h=28).reset_index()\n",
    "\n",
    "preds_sf = pd.read_parquet(...) # load in the saved predictions from the project data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting to ensemble, let's check the RMSSE of our individual models. Our modified version of RMSSE takes predictions in a rectangular shape, with each row being one `unique_id` and each column being one of the 28 predictions for the validation set. That results in an array of shape `[n_unique_ids, 28]`. We need to do the same thing for the training data (`y_insample` here) to create the scale (the denominator). Finally, we create a mask to tell the RMSSE function not to calculate the scale value before each `unique_id` has its first sale, since we don't calculate the loss over those periods.\n",
    "\n",
    "This code assumes you stored your `mlforecast` predictions in `preds_ml` and your `statsforecast` predictions in `preds_sf`.\n",
    "\n",
    "Modify it to suit your needs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = val.sort_values(['unique_id', 'ds']).y.values.reshape(-1, 28)\n",
    "y_hat_ml = preds_ml.sort_values(['unique_id', 'ds']).LGBMRegressor.values.reshape(-1, 28)\n",
    "y_hat_sf = preds_sf.sort_values(['unique_id', 'ds']).AutoETS.values.reshape(-1, 28)\n",
    "y_insample = train.sort_values(['unique_id', 'ds']).y.values.reshape(-1, 1210)\n",
    "insample_mask = (y_insample.cumsum(axis=1) > 0).astype(int)[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'LightGBM Tweedie RMSSE: {rmsse(y, y_hat_ml, y_insample, insample_mask=insample_mask)}')\n",
    "print(f'AutoETS RMSSE: {rmsse(y, y_hat_sf, y_insample, insample_mask=insample_mask)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `y_hat`, which is a NumPy array of all of your model's predictions combined along a new axis. `y_hat` should be of shape `[n_models, n_unique_ids, 28]`. So, if you only fit one `mlforecast` model and one `statsforecast` model, it'd be of shape `[2, n_unique_ids, 28]`.\n",
    "\n",
    "The reason we're doing this is because we're going to have a single weight for each model, and that weight will be between 0 and 1. So, that means we can take a weighted average across the first dimension using those weights to calculate our ensemble. But, that after this!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Hint: try running the following code:\n",
    "\n",
    "```\n",
    "arr = np.array([\n",
    "    [0, 1, 2],\n",
    "    [3, 4, 5],\n",
    "    [6, 7, 8],\n",
    "    [9, 0, 1],\n",
    "])\n",
    "print(arr.shape)\n",
    "print(arr[None, :].shape)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate `y_hat_avg` in `ensemble_metric`. `y_hat_avg` is a weighted average of `y_hat` along the first dimension, and is weighted according to `weights`, which is a list of floats of length `n_models`. There are some tests in there to help you out!\n",
    "\n",
    "Here's what you have to do:\n",
    "\n",
    "1. Finish the definition for `init_guess`, which should be a list with length equal to the number of models you trained. Initialize it such that the weights for each model are between 0 and 1 and are equal for every model.\n",
    "2. Run the code and make note of the RMSSE value. This is the RMSSE for a simple average of your predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def ensemble_metric(weights, y, y_hat, y_insample, insample_mask):\n",
    "    y_hat_avg = np.average(y_hat, axis=0, weights=weights)\n",
    "    assert y_hat_avg.ndim == 2, 'y_hat_avg has {y_hat_avg.ndim} dimensions, but it must be 2D. Did you calculate a weighted average over the first dimension?'\n",
    "    assert y_hat_avg.shape == y.shape, 'y_hat_avg and y must have the same shape. y_hat_avg has shape {y_hat_avg.shape}, but y has shape {y.shape}'\n",
    "    return rmsse(y, y_hat_avg, y_insample, insample_mask=insample_mask)\n",
    "\n",
    "ensemble_metric = partial(ensemble_metric, y=y, y_hat=y_hat, y_insample=y_insample, insample_mask=insample_mask)\n",
    "\n",
    "# Our first guess is setting all weights equal to each other, such that they sum up to 1\n",
    "init_guess = ...\n",
    "\n",
    "print(f'Inital Blend RMSSE: {ensemble_metric(init_guess):.6f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On line 16, replace `oof_names` with a list of the names of your models in the order that you added them to `y_hat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "bnds = [(0, 1) for _ in range(y_hat.shape[0])] # Weights must be between 0 and 1\n",
    "\n",
    "res_scipy = minimize(\n",
    "    fun=ensemble_metric, \n",
    "    x0=init_guess, \n",
    "    method='Powell', \n",
    "    bounds=bnds, \n",
    "    options=dict(maxiter=1_000_000),\n",
    "    tol=1e-8\n",
    ")\n",
    "\n",
    "print(f'Optimised Blend RMSSE: {res_scipy.fun:.6f}')\n",
    "print(f'Optimised Weights: {res_scipy.x}')\n",
    "print('-' * 70)\n",
    "\n",
    "oof_names = ...\n",
    "for n, key in enumerate(oof_names):\n",
    "    print(f'{key} Optimised Weights: {res_scipy.x[n]:.6f}')\n",
    "\n",
    "ws = [ res_scipy.x[i] for i in range(len(oof_names))]\n",
    "\n",
    "# normalize the weights so they sum to 1\n",
    "weights = ws / np.sum(ws)\n",
    "print(f'Normalized weights:')\n",
    "print(weights)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, answer the following questions:\n",
    "\n",
    "1. How did the accuracy of your single model compare to the accuracy of the optimized ensemble?\n",
    "2. How did the simple average ensemble compare to the optimized ensemble?\n",
    "3. Does it make sense how the models were weighted? Did the most accurate model get the largest weight?\n",
    "4. How might you combine hierarchical forecasting with ensembling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5768b04258121350f986a32a10c2b5b63ea833426012d4b5b8a887aeeef377c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
