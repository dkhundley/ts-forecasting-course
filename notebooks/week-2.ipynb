{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2\n",
    "\n",
    "We're going to continue from where we left off with Project 1. Project 1 left us with a daily time series for every product with no gaps -- exactly what we want for modeling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(f'{data_path}/sales_data.parquet')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did some EDA in Project 1, but it was primarily focused on higher level patterns (i.e., at the department level). This time, spend some time doing EDA at the item level to see what kind of items you're dealing with.\n",
    "\n",
    "Some questions you may want to explore:\n",
    "\n",
    "1. How do high-volume items compare to low-volume/itermittent items?\n",
    "2. What sort of seasonal patterns are at play?\n",
    "3. Do items from different departments show different patterns?\n",
    "4. Does the same item show different behavior at different stores?\n",
    "\n",
    "These questions are just a starting point. Feel free to explore this any way you feel is necessary to make better models. The best EDA is done iteratively, so I encourage you to come back to this once you've started fitting models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some EDA!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get to modeling, let's create our evaluation setup. The models that we're going to create have a 28-day forecast horizon, and our goal is to best approximate \"average\" sales."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to implement our evaluation metric. The original competition used a metric called RMSSE, or \"Root Mean Squared Scaled Error.\" It's similar to the MASE metric that we discussed before, except that the metric optimizes better for \"average\" sales (as opposed to MASE, which optimizes for the median, since it's an absolute error metric). The competition actually used a weighted version of RMSSE which is techincally more robust, but we're going to stick to RMSSE. Here's what RMSSE looks like:\n",
    "\n",
    "$RMSSE = \\sqrt{\\frac{1}{h}\\frac{\\sum^{n+h}_{t=n+1} (Y_t - \\hat{Y}_t)^2}{\\frac{1}{n-1}\\sum^n_{t=2} (Y_t - Y_{t-1})^2}}$\n",
    "\n",
    "where $Y_t$ is the actual future value of sales at date $t$, $\\hat{Y}_t$ is your forecast for date $t$, $n$ is the number of dates in our training set, and $h$ is our forecast horizon (28 days, in our case).\n",
    "\n",
    "That looks intimidating! But, similarly to MASE, you can break it down into two parts:\n",
    "- The numerator: $\\frac{1}{h}\\sum^{n+h}_{t=n+1} (Y_t - \\hat{Y}_t)^2$, which is just the MSE for every prediction in the validation set.\n",
    "- The denominator: $\\frac{1}{n-1}\\sum^n_{t=2} (Y_t - Y_{t-1})^2$, which is just the MSE over the entire training set if your forecast was a naive, one-day-ahead forecast. We refer to this as the \"scale\" since it's really just a benchmark -- errors less than this are better than the benchmark, and errors greater than this are worse.\n",
    "\n",
    "Of course, the \"naive, one-day-ahead forecast\" part only works if you calculate both the numerator and denominator separately for each `id`. So, the idea here is that you are effectively calculating an RMSSE value for each `id`, and then averaging those to get the final RMSSE.\n",
    "\n",
    "Last comment: there are products in the dataset that don't start showing sales for some time. For those products, the denominator is only supposed to be calculated after the first sale in the dataset. I'd recommend just dropping the records for those products until that first sales, which is straightforward to do using `.cumsum()` over `sales` while grouping by `id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION: filter out products that don't have sales using cumsum"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how you should implement your RMSSE:\n",
    "\n",
    "1. Create a function called `rmsse` that looks like this:\n",
    "\n",
    "`def rmsse(train, val, y_pred):`\n",
    "\n",
    "where:\n",
    "- `train` is the `pd.DataFrame` representing the training set\n",
    "- `val` is the `pd.DataFrame` representing the validation set\n",
    "- `y_pred` is either a `pd.Series` or `np.ndarray` that is the output of your model\n",
    "\n",
    "2. Start by calculating the scale (i.e. denominator from above) for each `id` over the training set.\n",
    "\n",
    "3. Then, calculate the MSE for each `id` over the validation set.\n",
    "\n",
    "4. Merge the scale dataframe onto the dataframe that contains your validation MSE values.\n",
    "\n",
    "5. Use the merged dataframe to calculate the RMSSE for each `id`, and finally return the average of all of those RMSSE values.\n",
    "\n",
    "Don't worry that you haven't split your data into training and validation sets yet. I gave you a test case below to see if your code is working before you move on. Also, don't be afraid to do this in a simple, looped fashion before refactoring it into more beautiful Pandas code. Take advantage of that test case!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION: implement rmsse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rmsse():\n",
    "    test_train = pd.DataFrame({\n",
    "        'id': ['a', 'a', 'a', 'b', 'b', 'b', 'c', 'c', 'c'],\n",
    "        'sales': [3, 2, 5, 100, 150, 60, 10, 20, 30],\n",
    "    })\n",
    "    test_val = pd.DataFrame({\n",
    "        'id': ['a', 'a', 'a', 'b', 'b', 'b', 'c', 'c', 'c'],\n",
    "        'sales': [6, 1, 4, 200, 120, 270, 10, 20, 30],\n",
    "    })\n",
    "    test_y_pred = pd.Series([1, 2, 3, 180, 160, 240, 20, 30, 40])\n",
    "\n",
    "    assert np.abs(rmsse(test_train, test_val, test_y_pred) - 0.92290404515501) < 1e-6\n",
    "\n",
    "test_rmsse()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point on, the project is a bit of a \"choose your own adventure.\" There's a huge range of skill levels out there, and I want to provide you with a path that will meet you where you're at (but test you a little). At a minimum, though, you'll be fitting a LightGBM model.\n",
    "\n",
    "1. If you're a beginner, use [`mlforecast`](https://nixtla.github.io/mlforecast/) (the sister package to `statsforecast`). It helps a lot with both feature engineering and model fitting, so you'll be able to try out a lot of options without getting bogged down in writing complex code. Focus your efforts on trying lots of different features/hyperparameters and seeing how they affect your model!\n",
    "\n",
    "If you want to go this route, here are the steps you should take:\n",
    "\n",
    "- Familiarize yourself with mlforecast [here](https://nixtla.github.io/mlforecast/)\n",
    "- Read the code in the below cell. This is your starting point!\n",
    "- Try adding other `date_features`, like the week of year and day of year.\n",
    "- Try adding `static_features=['item_id', 'dept_id', 'cat_id']` to `fcst.fit()`\n",
    "- Try out other rolling mean/std lengths and at different lags to see if they help. (You can import `rolling_std` from `window_ops.rolling`)\n",
    "- Try adding seasonal rolling means using the following code, which implements a 4 week seasonal rolling mean with a season length of 7 days:\n",
    "\n",
    "```\n",
    "@njit\n",
    "def seasonal_rolling_mean(x):\n",
    "    return seasonal_rolling_mean(x, season_length=7, window_size=4, min_samples=1)\n",
    "```\n",
    "- Try out some difference and lag features.\n",
    "- Try adding variables from the other data files, such as price.\n",
    "\n",
    "2. If you feel more comfortable, then I want you to not only try out different features/hyperparameters, but also compare modeling methods! Some things to try:\n",
    "\n",
    "- Features\n",
    "    - Benefits from lag features vs. rolling window features\n",
    "    - Which rolling window aggregations help\n",
    "    - Comparing seasonal rolling features to non-seasonal\n",
    "    - Features aggregated at the department/category level (but make sure to only calculate over the training set!)\n",
    "- Modeling\n",
    "    - Simple 28-day forecast horizon LightGBM model\n",
    "    - MLForecast's recursive strategy\n",
    "    - The multi-horizon strategy (i.e. one model predicting 7 days out, a second model predicting 14 days out, etc.)\n",
    "    - Deep learning models using [`neuralforecast`](https://nixtla.github.io/neuralforecast/) or `darts`\n",
    "\n",
    "3. (Optional) no matter which group you fit into, try adding in calendar and price features from the other data files that I added!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't worry about any error outputs here, unless you get the same \"Retrying\" error as Project 1\n",
    "! pip install mlforecast==0.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlforecast import MLForecast\n",
    "from statsforecast import StatsForecast\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from numba import njit\n",
    "from window_ops.rolling import rolling_mean\n",
    "import lightgbm as lgb\n",
    "\n",
    "# split into training and validation sets and conform the column names to what MLForecast expects\n",
    "val = (\n",
    "    data\n",
    "    .reset_index()\n",
    "    .groupby('id')\n",
    "    .tail(28)\n",
    "    .rename(columns={\n",
    "        'date': 'ds',\n",
    "        'id': 'unique_id',\n",
    "        'sales': 'y',\n",
    "    })\n",
    ")\n",
    "train = (\n",
    "    data\n",
    "    .reset_index()\n",
    "    .drop(val.index)\n",
    "    .rename(columns={\n",
    "        'date': 'ds',\n",
    "        'id': 'unique_id',\n",
    "        'sales': 'y',\n",
    "    })\n",
    ")\n",
    "\n",
    "# label encode categorical features\n",
    "cat_feats = ['unique_id', 'item_id', 'dept_id', 'cat_id']\n",
    "enc_cat_feats = [f'{feat}_enc' for feat in cat_feats]\n",
    "\n",
    "encoder = OrdinalEncoder()\n",
    "train[enc_cat_feats] = encoder.fit_transform(train[cat_feats])\n",
    "val[enc_cat_feats] = encoder.transform(val[cat_feats])\n",
    "\n",
    "reference_cols = ['unique_id', 'ds', 'y']\n",
    "\n",
    "# add features to this list if you want to use them\n",
    "features = reference_cols + enc_cat_feats\n",
    "train = train[features]\n",
    "val = val[features]\n",
    "\n",
    "@njit\n",
    "def rolling_mean_28(x):\n",
    "    return rolling_mean(x, window_size=28)\n",
    "\n",
    "# feel free to tweak these parameters!\n",
    "model_params = {\n",
    "    'verbose': -1,\n",
    "    'num_leaves': 256,\n",
    "    'n_estimators': 50,\n",
    "    'objective': 'tweedie',\n",
    "    'tweedie_variance_power': 1.1,\n",
    "}\n",
    "\n",
    "models = [\n",
    "    lgb.LGBMRegressor(**model_params),\n",
    "]\n",
    "\n",
    "\n",
    "fcst = MLForecast(\n",
    "    models=models,\n",
    "    freq='D',\n",
    "    # dictionary reads like this:\n",
    "    # {number of days to lag the feature: [list of functions to apply to the lagged data]}\n",
    "    lag_transforms={\n",
    "        7: [rolling_mean_28]\n",
    "    },\n",
    "    date_features=['dayofweek'],\n",
    ")\n",
    "\n",
    "# don't worry about nul value warnings. LightGBM and XGBoost can handle it!\n",
    "fcst.fit(\n",
    "    train, \n",
    "    id_col='unique_id', \n",
    "    time_col='ds', \n",
    "    target_col='y', \n",
    "    dropna=False\n",
    ")\n",
    "\n",
    "predictions = fcst.predict(28)\n",
    "\n",
    "# plot the last 45 days of the training set, the validation set, and the predictions\n",
    "plot_data = (\n",
    "    pd.concat([\n",
    "        train.groupby('unique_id').tail(45)[['unique_id', 'ds', 'y']], \n",
    "        val[['unique_id', 'ds', 'y']], \n",
    "        predictions\n",
    "    ])\n",
    ")\n",
    "\n",
    "# for some reason, MLForecast doesn't have this awesome plotting method!\n",
    "StatsForecast.plot(plot_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a brief summary of what helped your models and what didn't help. Was it what you expected?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5768b04258121350f986a32a10c2b5b63ea833426012d4b5b8a887aeeef377c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
